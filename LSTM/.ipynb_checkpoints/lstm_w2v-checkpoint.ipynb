{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amel\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import string\n",
    "import nltk\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_DATA = 'C:/Users/amel/Desktop/PFE/'\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "TEST_SPLIT = 0.2\n",
    "VALIDATION_SPLIT = 0.1\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training word2vec...\n",
      "['on', 'days', 'when', 'i', 'feel', 'close', 'to', 'my', 'partner', 'and', 'other', 'friends', 'when', 'i', 'feel', 'at', 'peace', 'with', 'myself', 'and', 'also', 'experience', 'a', 'close', 'contact', 'with', 'people', 'whom', 'i', 'regard', 'greatly']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amel\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "print('\\nTraining word2vec...')\n",
    "sentences, labels = loadData('data_sans_pr√©traitement.csv')\n",
    "words =[nltk.word_tokenize(sent) for sent in sentences]\n",
    "print(words[1])\n",
    "word_model = gensim.models.Word2Vec(words, size=100, min_count=1, window=5, iter=100)\n",
    "\n",
    "pretrained_weights = word_model.wv.syn0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, emdedding_size = pretrained_weights.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result embedding shape: (9047, 100)\n"
     ]
    }
   ],
   "source": [
    "print('Result embedding shape:', pretrained_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(filename):\n",
    "    df = pd.read_csv(DIR_DATA + filename,delimiter=';')\n",
    "    selected = ['label', 'text']\n",
    "    non_selected = list(set(df.columns) - set(selected))\n",
    "    df = df.drop(non_selected, axis=1)\n",
    "    df = df.dropna(axis=0, how='any', subset=selected)\n",
    "    labels = sorted(list(set(df[selected[0]].tolist())))\n",
    "    dict.fromkeys(set(df[selected[0]].tolist()))\n",
    "    label_dict = {}\n",
    "    for i in range(len(labels)):\n",
    "        label_dict[labels[i]] = i\n",
    "\n",
    "    x_train = df[selected[1]].apply(lambda x: clean_str(x)).tolist()\n",
    "    y_train = df[selected[0]].apply(lambda y: label_dict[y]).tolist()\n",
    "    y_train = to_categorical(np.asarray(y_train))\n",
    "    return x_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabAndData(sentences):\n",
    "    \n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    vocab = tokenizer.word_index\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return vocab,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstmModel(pretrained_weights,epoch):\n",
    "    model = Sequential()\n",
    "    n, embedding_dims = pretrained_weights.shape\n",
    "\n",
    "    model.add(Embedding(n, embedding_dims, weights=[pretrained_weights], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    model.add(LSTM(128, dropout=0.6, recurrent_dropout=0.6))\n",
    "    model.add(Dense(7))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_split=VALIDATION_SPLIT, epochs=epoch, batch_size=128)\n",
    "    model.save_weights('text_lstm_weights.h5')\n",
    "\n",
    "    scores= model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data created\n",
      "Train Test split\n",
      "WARNING:tensorflow:From C:\\Users\\amel\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\amel\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          904700    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 903       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 1,022,851\n",
      "Trainable params: 118,151\n",
      "Non-trainable params: 904,700\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\amel\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 5461 samples, validate on 607 samples\n",
      "Epoch 1/40\n",
      "5461/5461 [==============================] - 13s 2ms/step - loss: 1.9740 - acc: 0.1524 - val_loss: 1.8946 - val_acc: 0.2422\n",
      "Epoch 2/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.9181 - acc: 0.2066 - val_loss: 1.8563 - val_acc: 0.2784\n",
      "Epoch 3/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.8836 - acc: 0.2282 - val_loss: 1.8328 - val_acc: 0.2669\n",
      "Epoch 4/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.8596 - acc: 0.2578 - val_loss: 1.8037 - val_acc: 0.3015\n",
      "Epoch 5/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.8311 - acc: 0.2822 - val_loss: 1.7863 - val_acc: 0.2850\n",
      "Epoch 6/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.8199 - acc: 0.2714 - val_loss: 1.7755 - val_acc: 0.3213\n",
      "Epoch 7/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.8008 - acc: 0.2919 - val_loss: 1.7568 - val_acc: 0.3081\n",
      "Epoch 8/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.7941 - acc: 0.2999 - val_loss: 1.7436 - val_acc: 0.3196\n",
      "Epoch 9/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.7701 - acc: 0.3067 - val_loss: 1.7326 - val_acc: 0.3130\n",
      "Epoch 10/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.7474 - acc: 0.3234 - val_loss: 1.7130 - val_acc: 0.3344\n",
      "Epoch 11/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.7358 - acc: 0.3302 - val_loss: 1.7081 - val_acc: 0.3427\n",
      "Epoch 12/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.7213 - acc: 0.3338 - val_loss: 1.6995 - val_acc: 0.3311\n",
      "Epoch 13/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.7104 - acc: 0.3417 - val_loss: 1.6788 - val_acc: 0.3657\n",
      "Epoch 14/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.6913 - acc: 0.3435 - val_loss: 1.6658 - val_acc: 0.3674\n",
      "Epoch 15/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.6787 - acc: 0.3547 - val_loss: 1.6574 - val_acc: 0.3723\n",
      "Epoch 16/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.6643 - acc: 0.3576 - val_loss: 1.6490 - val_acc: 0.3773\n",
      "Epoch 17/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.6477 - acc: 0.3706 - val_loss: 1.6357 - val_acc: 0.3987\n",
      "Epoch 18/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.6364 - acc: 0.3809 - val_loss: 1.6304 - val_acc: 0.3987\n",
      "Epoch 19/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.6255 - acc: 0.3891 - val_loss: 1.6188 - val_acc: 0.3822\n",
      "Epoch 20/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.6134 - acc: 0.3915 - val_loss: 1.6141 - val_acc: 0.3954\n",
      "Epoch 21/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.6021 - acc: 0.3961 - val_loss: 1.5888 - val_acc: 0.4069\n",
      "Epoch 22/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.5823 - acc: 0.4019 - val_loss: 1.6073 - val_acc: 0.4036\n",
      "Epoch 23/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.5862 - acc: 0.4010 - val_loss: 1.5896 - val_acc: 0.4086\n",
      "Epoch 24/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.5699 - acc: 0.4138 - val_loss: 1.5667 - val_acc: 0.4366\n",
      "Epoch 25/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.5642 - acc: 0.4076 - val_loss: 1.5711 - val_acc: 0.4152\n",
      "Epoch 26/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.5335 - acc: 0.4296 - val_loss: 1.5776 - val_acc: 0.4217\n",
      "Epoch 27/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.5287 - acc: 0.4179 - val_loss: 1.5818 - val_acc: 0.4283\n",
      "Epoch 28/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.5152 - acc: 0.4318 - val_loss: 1.5581 - val_acc: 0.4498\n",
      "Epoch 29/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.4996 - acc: 0.4402 - val_loss: 1.5682 - val_acc: 0.4399\n",
      "Epoch 30/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.4863 - acc: 0.4512 - val_loss: 1.5727 - val_acc: 0.4514\n",
      "Epoch 31/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.4773 - acc: 0.4558 - val_loss: 1.5597 - val_acc: 0.4382\n",
      "Epoch 32/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.4627 - acc: 0.4576 - val_loss: 1.5546 - val_acc: 0.4333\n",
      "Epoch 33/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.4784 - acc: 0.4571 - val_loss: 1.5491 - val_acc: 0.4481\n",
      "Epoch 34/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.4625 - acc: 0.4648 - val_loss: 1.5694 - val_acc: 0.4547\n",
      "Epoch 35/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.4615 - acc: 0.4644 - val_loss: 1.5265 - val_acc: 0.4613\n",
      "Epoch 36/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.4338 - acc: 0.4642 - val_loss: 1.5331 - val_acc: 0.4646\n",
      "Epoch 37/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.4359 - acc: 0.4679 - val_loss: 1.5271 - val_acc: 0.4679\n",
      "Epoch 38/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.4122 - acc: 0.4759 - val_loss: 1.5164 - val_acc: 0.4712\n",
      "Epoch 39/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.4349 - acc: 0.4629 - val_loss: 1.5338 - val_acc: 0.4728\n",
      "Epoch 40/40\n",
      "5461/5461 [==============================] - 12s 2ms/step - loss: 1.4248 - acc: 0.4847 - val_loss: 1.5236 - val_acc: 0.4712\n",
      "acc: 49.87%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #sentences, labels = loadData('data_pr√©traitement.csv')\n",
    "    \n",
    "    vocab, data = createVocabAndData(sentences)\n",
    "    print (\"Data created\")\n",
    "    print(\"Train Test split\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=TEST_SPLIT, random_state=42)\n",
    "    model=lstmModel(pretrained_weights,40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
