{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_GLOVE = os.path.abspath('glove.840B.300d/')\n",
    "DIR_DATA ='C:/Users/amel/Desktop/PFE/EmotionModelTraining/DATA/'\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "TEST_SPLIT = 0.2\n",
    "VALIDATION_SPLIT = 0.1\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gloveVec(filename):\n",
    "    embeddings = {}\n",
    "    f = open(os.path.join(DIR_GLOVE, filename), encoding='utf-8')\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = coefs\n",
    "        except ValueError:\n",
    "            i += 1\n",
    "    f.close()\n",
    "    return embeddings\n",
    "embeddings = gloveVec('glove.840B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(filename):\n",
    "    df = pd.read_csv(DIR_DATA + filename,delimiter=';')\n",
    "    selected = ['label', 'text']\n",
    "    non_selected = list(set(df.columns) - set(selected))\n",
    "    df = df.drop(non_selected, axis=1)\n",
    "    df = df.dropna(axis=0, how='any', subset=selected)\n",
    "    labels = sorted(list(set(df[selected[0]].tolist())))\n",
    "    dict.fromkeys(set(df[selected[0]].tolist()))\n",
    "    label_dict = {}\n",
    "    for i in range(len(labels)):\n",
    "        label_dict[labels[i]] = i\n",
    "\n",
    "    x_train = df[selected[1]].apply(lambda x: clean_str(x)).tolist()\n",
    "    y_train = df[selected[0]].apply(lambda y: label_dict[y]).tolist()\n",
    "    y_train = to_categorical(np.asarray(y_train))\n",
    "    return x_train,y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabAndData(sentences):\n",
    "    \n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    vocab = tokenizer.word_index\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return vocab,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEmbeddingMatrix(word_index,embeddings_index):\n",
    "    nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "    embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "    out_of_vocabulary = set()\n",
    "    for word, i in word_index.items():\n",
    "        if i > MAX_NB_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            #pour afficher les mots qui se trouve dans le dataset et ne se trouve pas dans le dictionnaire du glove\n",
    "            #out_of_vocabulary est un tableau différent à l'autre fonction out_of_vocabulary(text)\n",
    "        if embedding_vector is None:\n",
    "            out_of_vocabulary.add(word)    \n",
    "    return embedding_matrix,out_of_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstmModel(embedding_matrix,epoch):\n",
    "    model = Sequential()\n",
    "    n, embedding_dims = embedding_matrix.shape\n",
    "\n",
    "    model.add(Embedding(n, embedding_dims, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    model.add(LSTM(128, dropout=0.6, recurrent_dropout=0.6))\n",
    "    model.add(Dense(7))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_split=VALIDATION_SPLIT, epochs=epoch, batch_size=128)\n",
    "    model.save_weights('text_lstm_weights.h5')\n",
    "\n",
    "    scores= model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data created\n",
      "Train Test split\n",
      "WARNING:tensorflow:From C:\\Users\\amel\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\amel\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          2715600   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 903       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 2,936,151\n",
      "Trainable params: 220,551\n",
      "Non-trainable params: 2,715,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\amel\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 5474 samples, validate on 609 samples\n",
      "Epoch 1/40\n",
      "5474/5474 [==============================] - 31s 6ms/step - loss: 1.8833 - acc: 0.2338 - val_loss: 1.7740 - val_acc: 0.3284\n",
      "Epoch 2/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 1.7441 - acc: 0.3297 - val_loss: 1.6303 - val_acc: 0.3777\n",
      "Epoch 3/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 1.6518 - acc: 0.3641 - val_loss: 1.5386 - val_acc: 0.4171\n",
      "Epoch 4/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 1.5984 - acc: 0.3919 - val_loss: 1.4825 - val_acc: 0.4401\n",
      "Epoch 5/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 1.5408 - acc: 0.4235 - val_loss: 1.4370 - val_acc: 0.4466\n",
      "Epoch 6/40\n",
      "5474/5474 [==============================] - 16s 3ms/step - loss: 1.4947 - acc: 0.4478 - val_loss: 1.3846 - val_acc: 0.4877\n",
      "Epoch 7/40\n",
      "5474/5474 [==============================] - 16s 3ms/step - loss: 1.4383 - acc: 0.4699 - val_loss: 1.3269 - val_acc: 0.5107\n",
      "Epoch 8/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 1.3641 - acc: 0.5066 - val_loss: 1.2938 - val_acc: 0.5271\n",
      "Epoch 9/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 1.3243 - acc: 0.5130 - val_loss: 1.2553 - val_acc: 0.5599\n",
      "Epoch 10/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 1.2880 - acc: 0.5316 - val_loss: 1.2311 - val_acc: 0.5550\n",
      "Epoch 11/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 1.2348 - acc: 0.5566 - val_loss: 1.2073 - val_acc: 0.5731\n",
      "Epoch 12/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 1.2081 - acc: 0.5667 - val_loss: 1.2009 - val_acc: 0.5616\n",
      "Epoch 13/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 1.1799 - acc: 0.5820 - val_loss: 1.1937 - val_acc: 0.5599\n",
      "Epoch 14/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 1.1505 - acc: 0.5976 - val_loss: 1.1740 - val_acc: 0.5632\n",
      "Epoch 15/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 1.1147 - acc: 0.6061 - val_loss: 1.1747 - val_acc: 0.5599\n",
      "Epoch 16/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 1.1060 - acc: 0.6058 - val_loss: 1.1699 - val_acc: 0.5747\n",
      "Epoch 17/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 1.0726 - acc: 0.6160 - val_loss: 1.1491 - val_acc: 0.5878\n",
      "Epoch 18/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 1.0758 - acc: 0.6166 - val_loss: 1.1592 - val_acc: 0.5829\n",
      "Epoch 19/40\n",
      "5474/5474 [==============================] - 14s 3ms/step - loss: 1.0303 - acc: 0.6370 - val_loss: 1.1448 - val_acc: 0.5944\n",
      "Epoch 20/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 1.0187 - acc: 0.6399 - val_loss: 1.1511 - val_acc: 0.5796\n",
      "Epoch 21/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 1.0069 - acc: 0.6423 - val_loss: 1.1423 - val_acc: 0.5928\n",
      "Epoch 22/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 0.9941 - acc: 0.6493 - val_loss: 1.1317 - val_acc: 0.5911\n",
      "Epoch 23/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 0.9865 - acc: 0.6443 - val_loss: 1.1326 - val_acc: 0.5961\n",
      "Epoch 24/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 0.9648 - acc: 0.6611 - val_loss: 1.1372 - val_acc: 0.5928\n",
      "Epoch 25/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 0.9561 - acc: 0.6624 - val_loss: 1.1498 - val_acc: 0.5911\n",
      "Epoch 26/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 0.9464 - acc: 0.6706 - val_loss: 1.1393 - val_acc: 0.5977\n",
      "Epoch 27/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 0.9185 - acc: 0.6723 - val_loss: 1.1257 - val_acc: 0.5977\n",
      "Epoch 28/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 0.9118 - acc: 0.6765 - val_loss: 1.1310 - val_acc: 0.5928\n",
      "Epoch 29/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 0.8889 - acc: 0.6845 - val_loss: 1.1353 - val_acc: 0.6043\n",
      "Epoch 30/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 0.8865 - acc: 0.6889 - val_loss: 1.1454 - val_acc: 0.5977\n",
      "Epoch 31/40\n",
      "5474/5474 [==============================] - 14s 3ms/step - loss: 0.8739 - acc: 0.6915 - val_loss: 1.1518 - val_acc: 0.5911\n",
      "Epoch 32/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 0.8649 - acc: 0.6911 - val_loss: 1.1519 - val_acc: 0.5977\n",
      "Epoch 33/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 0.8420 - acc: 0.7055 - val_loss: 1.1413 - val_acc: 0.5961\n",
      "Epoch 34/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 0.8358 - acc: 0.7048 - val_loss: 1.1448 - val_acc: 0.5993\n",
      "Epoch 35/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 0.8290 - acc: 0.7053 - val_loss: 1.1388 - val_acc: 0.6043\n",
      "Epoch 36/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 0.8167 - acc: 0.7148 - val_loss: 1.1484 - val_acc: 0.6059\n",
      "Epoch 37/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 0.8039 - acc: 0.7185 - val_loss: 1.1431 - val_acc: 0.6125\n",
      "Epoch 38/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 0.7938 - acc: 0.7249 - val_loss: 1.1551 - val_acc: 0.6158\n",
      "Epoch 39/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 0.7761 - acc: 0.7273 - val_loss: 1.1543 - val_acc: 0.6076\n",
      "Epoch 40/40\n",
      "5474/5474 [==============================] - 15s 3ms/step - loss: 0.7628 - acc: 0.7422 - val_loss: 1.1609 - val_acc: 0.6092\n",
      "acc: 64.43%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sentences, labels = loadData('data_sans_prétraitement.csv')\n",
    "    \n",
    "    vocab, data = createVocabAndData(sentences)\n",
    "    embedding_mat,out_of_vocabulary = createEmbeddingMatrix(vocab,embeddings)\n",
    "    pickle.dump([data, labels, embedding_mat], open('embedding_matrix.pkl', 'wb'))\n",
    "    print (\"Data created\")\n",
    "\n",
    "    print(\"Train Test split\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=TEST_SPLIT, random_state=42)\n",
    "\n",
    "    model=lstmModel(embedding_mat,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.sequential.Sequential object at 0x000000444DBA1B38>\n"
     ]
    }
   ],
   "source": [
    "# save the model to disk\n",
    "import pickle\n",
    "filename = os.path.abspath('lstm_model_sans_prétraitement.sav/')\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "print(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tokenizer to disk\n",
    "filename = 'C:/Users/amel/Desktop/PFE/tok_lstm_sans_prétraitement.sav'\n",
    "pickle.dump(tokenizer, open(filename, 'wb'))\n",
    "# load the model from disk\n",
    "loaded_tok = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
